<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title></title>
</head>
<body><h3 id='ai-transcription-showdown'>AI Transcription Showdown</h3>
<h1 id='title-comparing-whisper-models-for-transcribing-queens-dont-stop-me-now'>Title: Comparing Whisper Models for Transcribing Queen&#39;s &quot;Don&#39;t Stop Me Now&quot;</h1>
<h2 id='which-model-captures-the-energy-of-freddie-mercurys-classic-hit'>Which Model Captures the Energy of Freddie Mercury’s Classic Hit?</h2>
<p>Transcribing song lyrics from audio is no easy feat, especially for a high-octane track like Queen&#39;s <em>Don&#39;t Stop Me Now</em>, with its rapid vocals, vibrant vocalizations (e.g., &quot;ooh, ooh, ooh&quot;), and iconic &quot;La-da-da-da-dah&quot; ending. In this article, we put six Whisper models—<code>base</code>, <code>medium</code>, <code>medium-q8</code>, <code>large-v3</code>, <code>large-v3-turbo</code>, and <code>large-v3-turbo-q8_0</code>—to the test to find out which delivers the most accurate transcription. Using the Whisper.cpp framework on an Apple M1 Pro, we evaluate each model based on <strong>accuracy</strong>, <strong>completeness</strong>, <strong>fidelity</strong>, and <strong>processing time</strong>, offering insights for anyone tackling music transcription with AI.</p>
<h2 id='methodology'>Methodology</h2>
<p>We processed an MP4 file of <em>Don&#39;t Stop Me Now</em> (duration: 3:31) using each Whisper model, generating VTT files with transcribed lyrics. These transcriptions were compared against the official lyrics, focusing on three key criteria:</p>
<ul>
<li><strong>Accuracy</strong>: Correctness of wording, phrasing, and punctuation.</li>
<li><strong>Completeness</strong>: Inclusion of all song parts, including verses, choruses, vocalizations, and the ending.</li>
<li><strong>Fidelity</strong>: Preservation of stylistic elements, such as repetitions and vocalizations, to maintain the song’s energy and structure.</li>

</ul>
<p>Processing time was measured in minutes and seconds (converted from milliseconds) to assess efficiency. Model details, including size and architecture, were extracted from command-line outputs to provide context for performance differences.</p>
<h2 id='results'>Results</h2>
<p>Each model’s transcription was scrutinized for errors, omissions, and stylistic accuracy. Below, we summarize their performance and present a detailed ranking table for easy comparison.</p>
<h3 id='model-performance-overview'>Model Performance Overview</h3>
<figure class='table-figure'><table>
<thead>
<tr><th><strong>Model</strong></th><th><strong>Model Size (MB)</strong></th><th><strong>Audio Layers</strong></th><th><strong>Text Layers</strong></th><th><strong>Processing Time</strong></th><th><strong>Key Strengths</strong></th><th><strong>Key Weaknesses</strong></th></tr></thead>
<tbody><tr><td><strong>base</strong></td><td>147.37</td><td>6</td><td>6</td><td>0:06</td><td>Fastest processing</td><td>Major errors (e.g., &quot;Lady Gaga,&quot; &quot;Mr. Farron Pied&quot;), incorrect ending</td></tr><tr><td><strong>medium</strong></td><td>1533.14</td><td>24</td><td>24</td><td>0:21</td><td>Good structure</td><td>Errors (e.g., &quot;driving inside out&quot;), simplified ending</td></tr><tr><td><strong>medium-q8</strong></td><td>822.75</td><td>24</td><td>24</td><td>0:17</td><td>Balanced accuracy and efficiency</td><td>Minor errors (e.g., &quot;six machine&quot;), simplified ending</td></tr><tr><td><strong>large-v3</strong></td><td>3094.36</td><td>32</td><td>32</td><td>0:52</td><td>Highest accuracy</td><td>Extraneous repetitions, missing ending</td></tr><tr><td><strong>large-v3-turbo</strong></td><td>1623.92</td><td>32</td><td>4</td><td>0:17</td><td>Improved over q8_0</td><td>Incorrect ending (&quot;Thank you&quot;), missing vocalizations</td></tr><tr><td><strong>large-v3-turbo-q8_0</strong></td><td>873.55</td><td>32</td><td>4</td><td>0:14</td><td>Fast for large model</td><td>Missing vocalizations, incorrect ending</td></tr></tbody>
</table></figure>
<h3 id='detailed-evaluation'>Detailed Evaluation</h3>
<ol start='' >
<li><p><strong>large-v3</strong>:</p>
<ul>
<li><strong>Accuracy</strong>: Excels with minor errors, such as &quot;whoa&quot; instead of &quot;oh&quot; in &quot;explode&quot; and &quot;Mr. Fire Up High&quot; once instead of &quot;Mister Fahrenheit.&quot;</li>
<li><strong>Completeness</strong>: Captures most of the song but omits the &quot;La-da-da-da-dah&quot; ending. Includes extraneous repetitions of &quot;burning through the sky.&quot;</li>
<li><strong>Fidelity</strong>: Strong, preserving the song’s dynamic energy despite repetitions.</li>
<li><strong>Processing Time</strong>: Slowest at 52 seconds, reflecting its large size (3094.36 MB) and robust architecture (32 audio/text layers).</li>
<li><strong>Verdict</strong>: The gold standard for accuracy and fidelity, perfect for high-quality transcription needs.</li>

</ul>
</li>
<li><p><strong>medium-q8</strong>:</p>
<ul>
<li><strong>Accuracy</strong>: Very accurate, with minor errors like &quot;just while they call me Mr. Fahrenheit&quot; and &quot;six machine&quot; instead of &quot;sex machine.&quot;</li>
<li><strong>Completeness</strong>: Nearly complete, featuring a simplified &quot;Ah, da, da, da&quot; ending and minor vocalization omissions.</li>
<li><strong>Fidelity</strong>: Maintains the song’s structure and vibe with slight simplifications.</li>
<li><strong>Processing Time</strong>: Efficient at 17 seconds, leveraging its quantized model (822.75 MB).</li>
<li><strong>Verdict</strong>: A practical choice for balancing quality and speed.</li>

</ul>
</li>
<li><p><strong>medium</strong>:</p>
<ul>
<li><strong>Accuracy</strong>: Fairly accurate but hampered by errors like &quot;driving inside out&quot; instead of &quot;I&#39;ll turn it inside out&quot; and &quot;six machine.&quot;</li>
<li><strong>Completeness</strong>: Includes most of the song with an abbreviated &quot;La, la, la&quot; ending; simplifies vocalizations.</li>
<li><strong>Fidelity</strong>: Decent but misses some stylistic flourishes.</li>
<li><strong>Processing Time</strong>: 21 seconds, slower than <code>medium-q8</code> despite a larger size (1533.14 MB).</li>
<li><strong>Verdict</strong>: Solid but outperformed by <code>medium-q8</code> due to more errors.</li>

</ul>
</li>
<li><p><strong>large-v3-turbo</strong> (un-quantized):</p>
<ul>
<li><strong>Accuracy</strong>: Generally accurate, with errors like &quot;leaving&quot; instead of &quot;leaping&quot; and &quot;A battle. Oh, oh, oh, oh, oh, explode&quot; instead of &quot;about to oh, oh, oh, oh, oh, explode.&quot;</li>
<li><strong>Completeness</strong>: Misses vocalizations (e.g., &quot;hey, hey, hey,&quot; &quot;I like it&quot;) and the &quot;La-da-da-da-dah&quot; ending, replaced with &quot;Thank you.&quot;</li>
<li><strong>Fidelity</strong>: Captures some energy but feels rushed; the incorrect ending disrupts the song’s closure.</li>
<li><strong>Processing Time</strong>: 17 seconds, competitive with <code>medium-q8</code>.</li>
<li><strong>Verdict</strong>: Slightly better than its quantized counterpart but limited by key omissions.</li>

</ul>
</li>
<li><p><strong>large-v3-turbo-q8_0</strong>:</p>
<ul>
<li><strong>Accuracy</strong>: Similar to un-quantized, with errors like &quot;leaving&quot; and &quot;but I&#39;ll explode.&quot;</li>
<li><strong>Completeness</strong>: Lacks vocalizations and the ending, with a more condensed structure.</li>
<li><strong>Fidelity</strong>: Rushed and less precise than the un-quantized version.</li>
<li><strong>Processing Time</strong>: Fastest among larger models at 14 seconds.</li>
<li><strong>Verdict</strong>: Prioritizes speed but sacrifices detail.</li>

</ul>
</li>
<li><p><strong>base</strong>:</p>
<ul>
<li><strong>Accuracy</strong>: Poor, with major errors like &quot;Lady Gaga&quot; for &quot;Lady Godiva&quot; and &quot;Mr. Farron Pied&quot; for &quot;Mister Fahrenheit.&quot;</li>
<li><strong>Completeness</strong>: Incorrectly ends with &quot;(singing in foreign language)&quot;; misordered sections.</li>
<li><strong>Fidelity</strong>: Fails to capture the song’s structure or style.</li>
<li><strong>Processing Time</strong>: Fastest at 6 seconds, due to its small size (147.37 MB).</li>
<li><strong>Verdict</strong>: Unsuitable for music transcription.</li>

</ul>
</li>

</ol>
<h3 id='ranking-table'>Ranking Table</h3>
<figure class='table-figure'><table>
<thead>
<tr><th><strong>Rank</strong></th><th><strong>Model</strong></th><th><strong>Accuracy</strong></th><th><strong>Completeness</strong></th><th><strong>Fidelity</strong></th><th><strong>Processing Time</strong></th></tr></thead>
<tbody><tr><td><strong>1</strong></td><td><strong>large-v3</strong></td><td>Highly accurate; minor errors (&quot;whoa&quot; vs. &quot;oh,&quot; &quot;Mr. Fire Up High&quot; once).</td><td>Most of song; missing &quot;La-da-da-da-dah&quot;; extraneous repetitions.</td><td>Captures energy but disrupted by repetitions.</td><td>0:52</td></tr><tr><td><strong>2</strong></td><td><strong>medium-q8</strong></td><td>Very accurate; minor errors (&quot;just while...,&quot; &quot;six machine&quot;).</td><td>Nearly complete; simplified &quot;Ah, da, da, da&quot; ending; minor omissions.</td><td>Maintains structure with slight simplifications.</td><td>0:17</td></tr><tr><td><strong>3</strong></td><td><strong>medium</strong></td><td>Fairly accurate; errors (&quot;driving inside out,&quot; &quot;six machine&quot;).</td><td>Most of song; abbreviated &quot;La, la, la&quot; ending; simplified vocalizations.</td><td>Decent structure but loses some flourishes.</td><td>0:21</td></tr><tr><td><strong>4</strong></td><td><strong>large-v3-turbo</strong></td><td>Generally accurate; errors (&quot;leaving&quot; vs. &quot;leaping,&quot; &quot;A battle...&quot;).</td><td>Missing vocalizations and &quot;La-da-da-da-dah&quot; (&quot;Thank you&quot;); gap in bridge.</td><td>Captures some energy but rushed; incorrect ending.</td><td>0:17</td></tr><tr><td><strong>5</strong></td><td><strong>large-v3-turbo-q8_0</strong></td><td>Generally accurate; errors (&quot;leaving,&quot; &quot;but I&#39;ll explode&quot;).</td><td>Missing vocalizations and &quot;La-da-da-da-dah&quot;; condensed structure.</td><td>Captures some energy but rushed and less precise.</td><td>0:14</td></tr><tr><td><strong>6</strong></td><td><strong>base</strong></td><td>Major errors (&quot;Lady Gaga,&quot; &quot;Mr. Farron Pied&quot;).</td><td>Incomplete; incorrect &quot;(singing in foreign language)&quot; ending; misordered.</td><td>Fails to capture structure or style.</td><td>0:06</td></tr></tbody>
</table></figure>
<h2 id='insights-and-recommendations'>Insights and Recommendations</h2>
<ul>
<li><strong>Top Pick for Quality</strong>: The <code>large-v3</code> model is the clear winner for accuracy and fidelity, making it ideal for professional audio analysis or archival work. Its 52-second processing time is a trade-off for its superior results.</li>
<li><strong>Best All-Rounder</strong>: The <code>medium-q8</code> model strikes an excellent balance, delivering near-top-tier accuracy in just 17 seconds. It’s perfect for most users, especially on systems with limited resources.</li>
<li><strong>Speed Considerations</strong>: The <code>large-v3-turbo-q8_0</code> (14 seconds) and <code>base</code> (6 seconds) models are the fastest but compromise on quality. They’re suitable for quick drafts or less complex audio.</li>
<li><strong>Turbo Limitations</strong>: Both <code>large-v3-turbo</code> models (un-quantized and q8_0) prioritize speed with fewer text layers (4 vs. 32), but this leads to missing vocalizations and incorrect endings, making them less ideal for music transcription.</li>

</ul>
<h2 id='challenges-with-the-ending'>Challenges with the Ending</h2>
<p>The &quot;La-da-da-da-dah&quot; ending is a hallmark of <em>Don&#39;t Stop Me Now</em>, yet most models struggled:</p>
<ul>
<li><code>medium-q8</code> and <code>medium</code> approximated it as &quot;Ah, da, da, da&quot; and &quot;La, la, la,&quot; respectively, which is closer than others.</li>
<li><code>large-v3-turbo</code> and <code>large-v3-turbo-q8_0</code> replaced it with &quot;Thank you&quot; or omitted it, possibly misinterpreting audio artifacts.</li>
<li><code>large-v3</code> skipped it entirely, focusing on earlier sections.</li>
<li><code>base</code> labeled it &quot;(singing in foreign language),&quot; a significant error.</li>

</ul>
<p>This highlights a broader challenge in AI transcription: handling non-lyrical vocalizations and song endings, where models may misinterpret or oversimplify.</p>
<h2 id='conclusion'>Conclusion</h2>
<p>Transcribing <em>Don&#39;t Stop Me Now</em> showcased the strengths and limitations of Whisper models. The <code>large-v3</code> model leads for precision, while <code>medium-q8</code> offers a practical compromise for speed and quality. The <code>large-v3-turbo</code> models, though fast, stumble on lyrical complexity, and the <code>base</code> model is best avoided for music. Future improvements in handling vocalizations and endings could make these models even more powerful for music transcription.</p>
<p>Whether you’re a researcher, musician, or AI enthusiast, choose your model based on your needs: <code>large-v3</code> for top quality, <code>medium-q8</code> for versatility, or <code>large-v3-turbo-q8_0</code> for speed. Try them out and let us know your results!</p>
</body>
</html>
